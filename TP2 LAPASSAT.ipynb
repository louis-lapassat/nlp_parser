{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP TD 2 LAPASSAT Louis\n",
    "\n",
    "Link of the [GitHub (course)](https://github.com/edupoux/MVA_2020_SL/tree/master/TD_%232).\n",
    "\n",
    "**The goal of this assignment is to develop a basic probabilistic parser for French that is based on the CYK algorithm and the PCFG model and that is robust to unknown words.**\n",
    "\n",
    "The goal of this assignment is not to produce high-accuracy parsers. Its goal is rather for you to build a working statistical constituency parsing architecture based on the CYK algorithm, with a module for handling out-of-vocabulary words (OOVs) based on edit distance and word embedding similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PYEVALB # install evalb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here import the libraries\n",
    "import pickle\n",
    "import numpy as np\n",
    "from random import shuffle, seed\n",
    "from nltk import Tree, Nonterminal, induce_pcfg\n",
    "import re\n",
    "from pprint import pprint\n",
    "import operator\n",
    "import tqdm as tqdm\n",
    "from difflib import ndiff\n",
    "from PYEVALB import scorer\n",
    "from PYEVALB import parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "We use the SEQUOIA treebank v6.0 (file in the GitHub, bracketed format):\n",
    "\n",
    " * split it into 3 parts (80% / 10% / 10%).\n",
    " * use the 80% for training (extract CFG rules + learn CFG rule probabilities)\n",
    " * use the first 10% for development purposes (whatever you want to use it)\n",
    " * use the last 10% to evaluate your parser. To keep it simple, you can evaluate your part-of-speech accuracy only, i.e. via the percentage of tokens for which your parser choses the correct part-of-speech. For your information, a standard tool for constituency parse evaluation is [evalb](https://nlp.cs.nyu.edu/evalb/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    ### load the data\n",
    "    with open(path, \"r\", encoding='utf-8') as file: # open the file\n",
    "        content = file.readlines() # read line by line\n",
    "    content = [x.strip() for x in content]  # remove the '\\n' at the end of each line + ( ), [2:-1]\n",
    "    \n",
    "    ### now we drop functional labels\n",
    "    new_content = []\n",
    "    for sentence in content: # loop over all the sentence\n",
    "        matches = re.findall('-.+? ', sentence) # find all text starting from '-' and ending with ' '\n",
    "        matches = [match for match in matches if match[-2] != ')'] # remove false functional labels\n",
    "        for match in matches: # loop over the matches\n",
    "            sentence = re.sub(match, ' ', sentence) # remove the match text from the sentence\n",
    "        new_content.append(sentence) # save the treated sentence\n",
    "    return new_content\n",
    "\n",
    "def transform_to_sentence(data):\n",
    "    new_data = []\n",
    "    for sentence in data:\n",
    "        new_data.append(' '.join(Tree.fromstring(sentence).leaves()))\n",
    "    return new_data\n",
    "\n",
    "\n",
    "### get data\n",
    "content = load_data('sequoia-corpus+fct.txt')\n",
    "\n",
    "### split the data\n",
    "seed(42) # set the seed for random shuffle\n",
    "# shuffle(content) # first shuffle the data\n",
    "train_indice = int(len(content) * 0.8) # set the indice for the training set\n",
    "test_indice = int(len(content) * 0.1) + train_indice # set the indice for the testing set\n",
    "train, test, evaluation = content[:train_indice], content[train_indice:test_indice], content[test_indice:]\n",
    "\n",
    "### transform the data into sentence datasets\n",
    "train_sentences = transform_to_sentence(train)\n",
    "test_sentences = transform_to_sentence(test)\n",
    "evaluation_sentences = transform_to_sentence(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module to extract a PCFG\n",
    "\n",
    "module to extract a PCFG from the training corpus provided (see below), made of:\n",
    "\n",
    " * a probabilistic context-free grammar whose terminals are part-of-speech tags\n",
    " * a probabilistic lexicon, i.e. triples of the form (token, part-of-speech tag, probability) such that the sum of the probabilities for all triples for a given token sums to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcfg(dataset):\n",
    "    \"\"\"Extract the probabilistic context-free grammar from a dataset\"\"\"\n",
    "    productions = []\n",
    "    ### loop over trees (first convert bracketed string into tree)\n",
    "    for tree in [Tree.fromstring(tree, remove_empty_top_bracketing=True) for tree in train]:\n",
    "        tree.collapse_unary(collapsePOS=True)\n",
    "        tree.chomsky_normal_form(horzMarkov=1) # chomsky_normal_form(horzMarkov=2) # um_chomsky_normal_form() \n",
    "        productions += tree.productions() # add productions\n",
    "    starting_state = Nonterminal('SENT') # starting state, use tree.label() to find 'SENT'\n",
    "    grammar = induce_pcfg(starting_state, productions) # induce PCFG\n",
    "    return grammar\n",
    "\n",
    "def get_lexicon(grammar):\n",
    "    \"\"\"Extract probabilistic lexicon\"\"\"\n",
    "    dic = {}\n",
    "    for e in grammar.productions():\n",
    "        token = e.rhs()\n",
    "        part_of_speech_tag = e.lhs()\n",
    "        probability = e.prob()\n",
    "        if isinstance(part_of_speech_tag, Nonterminal):\n",
    "            if isinstance(token, tuple):\n",
    "                if isinstance(token[0], str):\n",
    "                    if token[0] in dic:\n",
    "                        if part_of_speech_tag in dic[token[0]]:\n",
    "                            print('nan mais allo quoi, error')\n",
    "                        else:\n",
    "                            dic_temp = dic[token[0]]\n",
    "                            dic[token[0]] = {part_of_speech_tag: probability, **dic_temp}\n",
    "                    else:\n",
    "                        dic = {token[0]: {part_of_speech_tag: probability}, **dic}\n",
    "            elif isinstance(token, str):\n",
    "                print('allo quoi, error')\n",
    "    \n",
    "    ### normalize per token\n",
    "    for token in dic:\n",
    "        total = sum(dic[token].values())\n",
    "        for key in dic[token]:\n",
    "            dic[token][key] /= total\n",
    "    return dic\n",
    "\n",
    "grammar = get_pcfg(train) # infer the grammer from the train \n",
    "lexicon = get_lexicon(grammar) # infer the lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 12706 productions in your grammar. Below is a quick look at some of them: \n",
      "\n",
      "[SENT -> NP+NPP [0.00242033],\n",
      " NP+NPP -> 'Gutenberg' [0.00247525],\n",
      " SENT -> NP SENT|<VN> [0.137959],\n",
      " NP -> DET NC [0.220609],\n",
      " DET -> 'Cette' [0.00236469],\n",
      " NC -> 'exposition' [0.00120797],\n",
      " SENT|<VN> -> VN SENT|<Ssub> [0.0439952],\n",
      " VN -> CLO V [0.0189378],\n",
      " CLO -> 'nous' [0.06],\n",
      " V -> 'apprend' [0.000483325]]\n"
     ]
    }
   ],
   "source": [
    "print(f'There is {len(grammar.productions())} productions in your grammar. Below is a quick look at some of them: \\n')\n",
    "pprint(grammar.productions()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 8958 keys in your lexicon. Here is a quick look at him:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prouvé': {VPP: 1.0},\n",
       " 'affirmait': {VN+V: 1.0},\n",
       " 'daté': {VPP: 1.0},\n",
       " 'valider': {VN+VINF: 1.0},\n",
       " 'filières': {NC: 1.0},\n",
       " 'complémentarité': {NC: 1.0},\n",
       " 'Soulignant': {VN+VPR: 1.0},\n",
       " 'Tiananmen': {NP+NPP: 1.0},\n",
       " 'rencontrés': {VPP: 1.0},\n",
       " 'au_cours_du': {P+D: 1.0}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'There is {len(lexicon)} keys in your lexicon. Here is a quick look at him:\\n')\n",
    "dict(list(lexicon.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOV module\n",
    "\n",
    "OOV module that assigns a (unique) part-of-speech to any token not included in the lexicon extracted from the training corpus. The underlying idea is to assign to an OOV the part-of-speech of a \"similar\" word. This similarity will be computed as a combination of formal similarity (to handle spelling errors) and embedding similarity (as measured by cosine similarity, i.e. scalar product between normalised vectors), to handle both spelling errors and genuine unknown words; you must design a reasonable way to combine these two similarities. For embedding similarity, you will use [the Polyglot embedding lexicon for French](https://sites.google.com/site/rmyeid/projects/polyglot) (see the [tutorial on Polyglot embeddings](https://nbviewer.jupyter.org/gist/aboSamoor/6046170); you can re-use this code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "\n",
    "class OOV():\n",
    "    def __init__(self, filepath, lexicon):\n",
    "        ### load words + embeddings\n",
    "        self.words, self.embeddings = pickle.load(open(filepath, 'rb'), encoding='latin1')\n",
    "        self.embeddings_norm = [np.linalg.norm(norm) for norm in self.embeddings]\n",
    "        self.word2id = {word: i for i, word in enumerate(self.words)}\n",
    "        print(f\"There is {len(self.words)} unique words and embddings shape is {self.embeddings.shape}.\")\n",
    "        \n",
    "        ### save lexicon\n",
    "        self.lexicon = lexicon\n",
    "        \n",
    "    def assign_part_of_speech_unknow(self, unknown_word):\n",
    "        ### get similar words\n",
    "        max_words = 50\n",
    "        most_similar_words = self.most_similar_words(unknown_word, k=max_words)\n",
    "        \n",
    "        ### loop over all similar words and try to get a match with lexicon\n",
    "        i = 0\n",
    "        similar_word = most_similar_words[i]\n",
    "        while similar_word not in self.lexicon and i < max_words:\n",
    "            similar_word = most_similar_words[i]\n",
    "            i += 1\n",
    "        \n",
    "        ### if we didn't find anything return error\n",
    "        if i >= max_words:\n",
    "            return None\n",
    "        \n",
    "        ### get the most probable nonterminal\n",
    "        pred_nonterminal = max(lexicon[similar_word].items(), key=operator.itemgetter(1))[0]\n",
    "        \n",
    "        return pred_nonterminal\n",
    "        \n",
    "    def cosine_similarity(self, word1, word2):\n",
    "        ### Return the cosine similarity\n",
    "        vec_word_1 = self.embeddings[self.word2id[word1]]\n",
    "        vec_word_2 = self.embeddings[self.word2id[word2]]\n",
    "        vec_word_1_norm = self.embeddings_norm[self.word2id[word1]]\n",
    "        vec_word_2_norm = self.embeddings_norm[self.word2id[word2]]\n",
    "        return np.dot(vec_word_1, vec_word_2) / (vec_word_1_norm * vec_word_2_norm)\n",
    "    \n",
    "    def levenshtein_distance(self, str1, str2):\n",
    "        counter = {\"+\": 0, \"-\": 0}\n",
    "        distance = 0\n",
    "        for edit_code, *_ in ndiff(str1, str2): # just take - or + (the code)\n",
    "            if edit_code == \" \":\n",
    "                distance += max(counter.values())\n",
    "                counter = {\"+\": 0, \"-\": 0}\n",
    "            else: \n",
    "                counter[edit_code] += 1\n",
    "        distance += max(counter.values())\n",
    "        return distance\n",
    "    \n",
    "    def damereau_levenshtein_distance(self, s1, s2):\n",
    "        len_s1 = len(s1)\n",
    "        len_s2 = len(s2)\n",
    "        m = np.zeros((len_s1, len_s2), dtype=int)\n",
    "        m[:, 0] = range(len_s1)\n",
    "        m[0, :] = range(len_s2)\n",
    "        for i in range(1, len_s1):\n",
    "            for j in range(1, len_s2):\n",
    "                if s1[i] == s2[j]:\n",
    "                    m[i, j] = min(m[i - 1, j], m[i, j - 1], m[i - 1, j - 1] - 1) + 1\n",
    "                else:\n",
    "                    m[i, j] = min(m[i - 1, j], m[i, j - 1], m[i - 1, j - 1]) + 1\n",
    "        return m[-1, -1]\n",
    "    \n",
    "    def most_similar_words(self, unknown_word, damereau=True, speed=True, k=5):\n",
    "        ### compute the score\n",
    "        if unknown_word in self.words:\n",
    "            res_cos = [self.cosine_similarity(unknown_word, word) for word in self.words]\n",
    "        else:\n",
    "            res_cos = [0 for word in self.words]\n",
    "        if damereau:\n",
    "            if speed and unknown_word in self.words:\n",
    "                upper_limit = np.quantile(res_cos, 0.8)\n",
    "                res_lev = []\n",
    "                for i, word in enumerate(self.words):\n",
    "                    if res_cos[i] < upper_limit:\n",
    "                        res_lev.append(100)\n",
    "                    else:\n",
    "                        res_lev.append(self.damereau_levenshtein_distance(unknown_word, word))\n",
    "            else:\n",
    "                res_lev = [self.damereau_levenshtein_distance(unknown_word, word) for word in self.words]\n",
    "        else:\n",
    "            res_lev = [self.levenshtein_distance(unknown_word, word) for word in self.words]\n",
    "        \n",
    "        ### scale in -1 to 1\n",
    "        res_lev = - np.array(res_lev) # - because if lev dist is big it is bad\n",
    "        res_lev = 2 * ((res_lev - res_lev.min()) / (res_lev.max() - res_lev.min())) - 1 # to [-1, 1]\n",
    "        \n",
    "        ### average \n",
    "        scores = 0.5 * res_lev + 0.5 * np.array(res_cos)\n",
    "        \n",
    "        ### get top k words\n",
    "        indice_top_k = np.argsort(scores)[::-1][1:k + 1]\n",
    "        \n",
    "        return [self.words[i] for i in indice_top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 100004 unique words and embddings shape is (100004, 64).\n"
     ]
    }
   ],
   "source": [
    "oov = OOV('polyglot-fr.pkl', lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trés', 'trop', 'très', 'assez', 'infiniment']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov.most_similar_words('tres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['être', \"m'être\", 'estre', 'ete', \"qu'être\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov.most_similar_words('etre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VINF"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov.assign_part_of_speech_unknow('etre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CYK algorithm\n",
    "\n",
    "probabilistic implementation of the CYK algorithm that takes tokenised sentences as an input. In other words, the input of the parser are files with one sentence per line, and each sentence is formed of tokens separated from one another by whitespace characters. The output should be in the same bracketed format as the training data. Regarding the probabilistic part, you will adapt the CYK algorithm so that only the best (i.e. most probable) way to rewrite an instanciated non-terminal symbol is retained. This will give you a recursive, straigtforward way to then retrieve the best (i.e. most probable) parse tree for the whole sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_lexicon(lexicon):\n",
    "    ### part_of_speech: {word: proba, etc.}\n",
    "    dic = {}\n",
    "    for key in lexicon:\n",
    "        for X in lexicon[key]:\n",
    "            if X in dic:\n",
    "                dic[X] = {key: lexicon[key][X], **dic[X]}\n",
    "            else:\n",
    "                dic[X] = {key: lexicon[key][X]}\n",
    "    return dic\n",
    "\n",
    "def get_rules(grammar):\n",
    "    is_nonter = lambda x: isinstance(x[0], Nonterminal) and isinstance(x[1], Nonterminal)\n",
    "    dic, dic_proba = {}, {}\n",
    "    for production in grammar.productions():\n",
    "        if production.lhs() in dic:\n",
    "            if len(production.rhs()) > 1:\n",
    "                if is_nonter(production.rhs()):\n",
    "                    dic[production.lhs()].append(production.rhs())\n",
    "                    dic_proba[production.lhs()] = {production.rhs(): production.prob(), **dic_proba[production.lhs()]}\n",
    "        else:\n",
    "            if len(production.rhs()) > 1:\n",
    "                if is_nonter(production.rhs()):\n",
    "                    dic[production.lhs()] = [production.rhs()]\n",
    "                    dic_proba[production.lhs()] = {production.rhs(): production.prob()}\n",
    "    return dic, dic_proba\n",
    "\n",
    "def get_tree(ch, bp, i, j, X):\n",
    "    if i == j:\n",
    "          return \"\".join([str(X),' ', ch[i]])\n",
    "    else:\n",
    "        try:\n",
    "            Y, Z, s = bp[i, j, X]\n",
    "            return \"\".join([str(X),' (', get_tree(ch, bp, i, s, Y),') (', get_tree(ch, bp, s+1, j, Z),')'])\n",
    "        except:\n",
    "            return \"\".join([str(X),' ', ch[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyk(sentence, lexicon, grammar):\n",
    "    inversed_lexicon = inverse_lexicon(lexicon)\n",
    "    n = len(sentence)\n",
    "    pi = {} \n",
    "    bp = {}\n",
    "    NonTerminals = set({a.lhs() for a in grammar.productions()}) # set (dic)\n",
    "    rules, rules_prob = get_rules(grammar)\n",
    "\n",
    "    ### Init\n",
    "    for i in range(n):\n",
    "        if sentence[i] in lexicon: # check if we already see the word\n",
    "            terminal = sentence[i]\n",
    "            for X in inversed_lexicon: # get probability          \n",
    "                if X in lexicon[terminal]: \n",
    "                    pi[i, i, X] = lexicon[terminal][X]\n",
    "                else:\n",
    "                    pi[i, i, X] = 0\n",
    "        else:\n",
    "            terminal = oov.assign_part_of_speech_unknow(sentence[i]) # if unknown get a similar pos\n",
    "            for X in inversed_lexicon: # get probability          \n",
    "                if X == terminal: \n",
    "                    pi[i, i, X] = 0.99\n",
    "                else:\n",
    "                    pi[i, i, X] = 0\n",
    "\n",
    "    ## main loop\n",
    "    for l in range(n - 1):\n",
    "        for i in range(n - l - 1): \n",
    "            j = i + l + 1\n",
    "            for X in rules: ## 40\n",
    "                max_score = 0\n",
    "                best_rule = None\n",
    "                for s in range(i, j): ## ok\n",
    "                    for Y, Z in rules[X]:  \n",
    "                        if not (i, s, Y) in pi or pi[i,s,Y]==None:\n",
    "                            pi[i,s,Y] = 0\n",
    "                        if not (s+1,j,Z) in pi or pi[s+1,j,Z]==None:\n",
    "                            pi[s+1,j,Z] = 0 ### ok\n",
    "\n",
    "                        score = rules_prob[X][(Y, Z)] * pi[i, s, Y] * pi[s + 1, j, Z]\n",
    "\n",
    "                        if max_score < score:\n",
    "                            max_score = score\n",
    "                            best_rule = Y, Z, s\n",
    "\n",
    "                bp[i, j, X] = best_rule\n",
    "                pi[i, j, X] = max_score\n",
    "\n",
    "    ### re-construction\n",
    "    max_score = 0\n",
    "    best_rule = None\n",
    "\n",
    "    for i,j,X in pi: \n",
    "        if i==0 and j== n-1:\n",
    "            if max_score < pi[0, n-1, X]:\n",
    "                max_score = pi[0, n-1, X]\n",
    "                best_rule = 0, n-1, X\n",
    "\n",
    "    if best_rule == None:\n",
    "        prediction = \"((SENT (NA error)))\"\n",
    "    else:\n",
    "        prediction = \"\".join(['((SENT (', get_tree(sentence, bp, *best_rule),')))'])\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d32f95ee1b6455badd2d9b6cd1526cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=311), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not correctly parsed.\n",
      "\n",
      "\n",
      "Recall =0.7692307692307693\n",
      "Precision =0.43478260869565216\n",
      "\n",
      "\n",
      "Recall =0.75\n",
      "Precision =0.391304347826087\n",
      "\n",
      "\n",
      "Recall =0.7333333333333333\n",
      "Precision =0.5238095238095238\n",
      "\n",
      "\n",
      "Not correctly parsed.\n",
      "\n",
      "\n",
      "Not correctly parsed.\n",
      "\n",
      "\n",
      "Recall =0.7333333333333333\n",
      "Precision =0.44\n",
      "\n",
      "\n",
      "Recall =0.4583333333333333\n",
      "Precision =0.34375\n",
      "\n",
      "\n",
      "Recall =0.375\n",
      "Precision =0.2222222222222222\n",
      "\n",
      "\n",
      "Not correctly parsed.\n",
      "\n",
      "\n",
      "Recall =0.30434782608695654\n",
      "Precision =0.2413793103448276\n",
      "\n",
      "\n",
      "Not correctly parsed.\n",
      "\n",
      "\n",
      "Not correctly parsed.\n",
      "\n",
      "\n",
      "Recall =0.4166666666666667\n",
      "Precision =0.29411764705882354\n",
      "\n",
      "\n",
      "Recall =0.46153846153846156\n",
      "Precision =0.35294117647058826\n",
      "\n",
      "\n",
      "Recall =0.4444444444444444\n",
      "Precision =0.3333333333333333\n",
      "\n",
      "\n",
      "Recall =0.4\n",
      "Precision =0.32\n",
      "\n",
      "\n",
      "Recall =0.3333333333333333\n",
      "Precision =0.3333333333333333\n",
      "\n",
      "\n",
      "Recall =0.6666666666666666\n",
      "Precision =0.6666666666666666\n",
      "\n",
      "\n",
      "Not correctly parsed.\n",
      "\n",
      "\n",
      "Not correctly parsed.\n",
      "\n",
      "\n",
      "Not correctly parsed.\n",
      "\n",
      "\n",
      "Not correctly parsed.\n",
      "\n",
      "\n",
      "Not correctly parsed.\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-76433a2b756a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m### make prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcyk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlexicon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrammar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m### compute score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-38f87b2c4760>\u001b[0m in \u001b[0;36mcyk\u001b[1;34m(sentence, lexicon, grammar)\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m## ok\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpi\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m                             \u001b[0mpi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpi\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\grammar.py\u001b[0m in \u001b[0;36m__eq__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \"\"\"\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_symbol\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_symbol\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__ne__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predicted_sentences = []\n",
    "precisions = []\n",
    "for i in tqdm.tqdm_notebook(range(len(evaluation))):\n",
    "    ### get data\n",
    "    sentence = evaluation_sentences[i].split(' ')\n",
    "    true = evaluation[i]\n",
    "    \n",
    "    ### make prediction\n",
    "    prediction = cyk(sentence, lexicon, grammar)\n",
    "    \n",
    "    ### compute score\n",
    "    true_tree = parser.create_from_bracket_string(true[1:-1])\n",
    "    pred_tree = parser.create_from_bracket_string(prediction[1:-1])\n",
    "    s = scorer.Scorer()\n",
    "    try:\n",
    "        result = s.score_trees(true_tree, pred_tree)\n",
    "        print('Recall =' + str(result.recall))\n",
    "        print('Precision =' + str(result.prec))\n",
    "        res_temp = result.recall\n",
    "    except:\n",
    "        print('Not correctly parsed.')\n",
    "        res_temp = 0\n",
    "    print('\\n')\n",
    "    \n",
    "    ### store the result\n",
    "    predicted_sentences.append(prediction)\n",
    "    precisions.append(res_temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
